{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Compression and Generation using Variational Autoencoders in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Intermediate-level knowledge of Python 3 (dealing with images and vector maths is useful)\n",
    "- Exposure to PyTorch usage\n",
    "- Basic understanding of Autoencoders and Generative Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Outline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1**: Introduction (this section)\n",
    "\n",
    "**Task 2**: Exploratory Data Analysis and Preprocessing\n",
    "\n",
    "**Task 3**: Training/Validation Split\n",
    "\n",
    "**Task 4**: Creating Data Loaders\n",
    "\n",
    "**Task 5**: VAE Architecture and Model Creation\n",
    "\n",
    "**Task 6**: Training Loop\n",
    "\n",
    "**Task 7**: Results and Other Uses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will explain the VAE briefly, but for additional information, feel free to check out the following resources:\n",
    "\n",
    "- [Resource 1: Blog Post](https://www.jeremyjordan.me/variational-autoencoders/)\n",
    "- [Resource 2: Theory Post](https://ermongroup.github.io/cs228-notes/inference/variational/)\n",
    "- [Resource 3: Lecture Video](https://www.youtube.com/watch?v=P78QYjWh5sM&feature=youtu.be)\n",
    "\n",
    "In addition, our VAE model in PyTorch has been adapted from the official PyTorch implementation you can find in [this repo](https://github.com/pytorch/examples/tree/master/vae). Enjoy the project!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/vae.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "random.seed(5)\n",
    "\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "batch_size = 64\n",
    "torch.manual_seed(17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Exploratory Data Analysis and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the computer-generated fonts of the [Character Font Images Data Set](http://archive.ics.uci.edu/ml/datasets/Character+Font+Images#)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def show_img(img):\n",
    "    img = img.permute(1, 2, 0)\n",
    "    if img.shape[2]==1:\n",
    "        img = img.view(img.shape[0], img.shape[1])\n",
    "    plt.title(f'Image has size {img.cpu().numpy().shape}')\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "transforms_all = transforms.Compose([])\n",
    "\n",
    "dummy_batch = torch.utils.data.DataLoader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "for batch in dummy_batch:\n",
    "    original_image = batch[0][0]\n",
    "    show_img(original_image)\n",
    "    show_img(transforms_all(transforms.ToPILImage()(original_image)))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Training/Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "folders = os.listdir('Font/all')\n",
    "\n",
    "# ignore hidden files\n",
    "folders = [folder for folder in folders if folder[0]!='.']\n",
    "\n",
    "os.mkdir('Font/train')\n",
    "os.mkdir('Font/val')\n",
    "\n",
    "for folder in tqdm (folders):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Creating Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "transforms_set = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize(size=50),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: VAE Architecture and Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        # need to fill in dimensions here\n",
    "    \n",
    "        z = 10000000000\n",
    "        \n",
    "        self.fc1 = nn.Linear(,)\n",
    "        self.fc21 = nn.Linear(,)\n",
    "        self.fc22 = nn.Linear(,)\n",
    "        self.fc3 = nn.Linear(,)\n",
    "        self.fc4 = nn.Linear(,)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        return torch.sigmoid(self.fc4(h3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # don't forget forward pass re-index\n",
    "        \n",
    "        mu, logvar = self.encode(x.view(-1, ???))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    \n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 50*50), reduction='sum')\n",
    "\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6: Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(evaluate_data=val_loader):\n",
    "    \n",
    "    \n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (data, _) in enumerate(evaluate_data):\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            if i == 0:\n",
    "                n = min(data.size(0), 16)\n",
    "                comparison = torch.cat([data[:n],\n",
    "                                        recon_batch.view(batch_size, 1, 50, 50)[:n]])\n",
    "                save_image(comparison.cpu(),\n",
    "                           'Results/reconstruction_' + str(epoch) + '.pdf', nrow=n)\n",
    "\n",
    "    val_loss /= len(evaluate_data.dataset)\n",
    "    return val_loss\n",
    "\n",
    "\n",
    "def sample_latent_space(epoch):\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # how to sample from our latent space\n",
    "        \n",
    "        save_image(sample.view(64, 1, 50, 50),\n",
    "                   'Results/sample_' + str(epoch) + '.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    progress_bar = tqdm(train_loader, desc='Epoch {:03d}'.format(epoch), leave=False, disable=False)\n",
    "    for data, _ in progress_bar:\n",
    "        \n",
    "        \n",
    "\n",
    "        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item() / len(data))})\n",
    "\n",
    "    average_train_loss = train_loss / len(train_loader.dataset)\n",
    "    tqdm.write('Training set loss (average, epoch {:03d}): {:.3f}'.format(epoch, average_train_loss))\n",
    "    val_loss = evaluate(val_loader)\n",
    "    tqdm.write('\\t\\t\\t\\t====> Validation set loss: {:.3f}'.format(val_loss))\n",
    "\n",
    "    train_losses.append(average_train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    if epoch%50==0:\n",
    "        torch.save(model.state_dict(), f'Models/epoch_{epoch}.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "np.savetxt('Models/training_losses.txt', np.array(train_losses), delimiter='\\n')\n",
    "np.savetxt('Models/validation_losses.txt', np.array(val_losses), delimiter='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7: Results and Other Uses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "len(tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plt.title('VAE Font Training')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After 950 epochs: \n",
    "<img src=\"Presaved_Results/reconstruction_950.png\">\n",
    "\n",
    "After 950 epochs: \n",
    "<img src=\"Presaved_Results/sample_950.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative Font Reconstruction (2-D Latent Space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After 1000 epochs: <img src=\"Presaved_Results/reconstruction_1000_2dim.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CelebA Reconstructions with 500-D Latent Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[CelebA Dataset](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html)\n",
    "\n",
    "After 100 epochs:\n",
    "<img src=\"Celebs/reconstruction_100.png\">\n",
    "\n",
    "After 200 epochs:\n",
    "<img src=\"Celebs/reconstruction_200.png\">\n",
    "\n",
    "After 300 epochs:\n",
    "<img src=\"Celebs/reconstruction_300.png\">\n",
    "\n",
    "After 400 epochs:\n",
    "<img src=\"Celebs/reconstruction_400.png\">\n",
    "\n",
    "After 500 epochs:\n",
    "<img src=\"Celebs/reconstruction_500.png\">\n",
    "\n",
    "After 600 epochs:\n",
    "<img src=\"Celebs/reconstruction_600.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
